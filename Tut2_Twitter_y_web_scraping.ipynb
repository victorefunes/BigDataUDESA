{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tut2 - Twitter_y_web_scraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorefunes/BigDataUDESA/blob/master/Tut2_Twitter_y_web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fSDMvhVwbvVk"
      },
      "source": [
        "# **TWITTER Y WEB SCRAPING**\n",
        "\n",
        "\n",
        "**Profesor**: Walter Sosa Escudero\n",
        "\n",
        "**Tutora**: Carla Srebot\n",
        "\n",
        "**Objetivo**: Aprender qué es una API, usarla para bajar tweets y aprender a hacer web scraping usando Beautiful Soup.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_a93ojPt-eom"
      },
      "source": [
        "## Bajando tweets\n",
        "\n",
        "En el código que sigue veremos cómo bajar tweets de manera sencilla\n",
        "\n",
        "### Tweepy\n",
        "Para más información, visitar:  https://readthedocs.org/projects/tweepy/downloads/pdf/latest/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8MKkj31jPuHC",
        "colab": {}
      },
      "source": [
        "# Defimos directorio de trabajo\n",
        "import os  \n",
        "dir = 'C:\\\\Users\\\\csreb\\\\Dropbox\\\\Tutoriales\\\\Big Data 2020\\\\Tutoriales\\\\Tutorial 2 - Web Scraping'\n",
        "os.getcwd()  \n",
        "os.chdir(dir)  \n",
        "\n",
        "# Para escribir archivos csv\n",
        "import csv\n",
        "\n",
        "# Para bajar tweets: \n",
        "# Primero, vamos a necesitar instalar el paquete llamada \"tweepy\" - esto lo pueden hacer directamente desde la terminal de Spyder\n",
        "# pip install tweepy\n",
        "import tweepy\n",
        "\n",
        "#Credenciales de Twitter\n",
        "consumer_key = '2MlfpmBC4d8zzz1cZxM1DhxMm'\n",
        "consumer_secret = 'QxcmeHECRZhGRZ0HqkNglxT5SOAg3y4jRnGaGw6j2fxOmM3f16'\n",
        "access_key = \"1158370347089113088-dTsbvEWhDnKBwSpSXG9MnHHIn7HKHl\"\n",
        "access_secret = \"0QxtTRAfl6FLuIuBQxFUPQ3JxdnyBfjuGcZkDUowIYW7S\"\n",
        "\n",
        "#Buscamos los útimos tweets de alguien\n",
        "def get_tweets(username):\n",
        "    \n",
        "    # Twitter requiere que todas las solicitudes utilicen OAuth para la autenticación:\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_key, access_secret)\n",
        "    # Entonces, ahora que tenemos nuestro OAuthHandler equipado con un token de acceso \n",
        "    api = tweepy.API(auth)\n",
        "\n",
        "    #¿Cuántos tweets? - ¡cuidado con el número de tweets!\n",
        "    number_of_tweets = 100\n",
        "\n",
        "    #¡Tomemos esos tweets!\n",
        "    tweets_for_csv = []\n",
        "    # A continuación, vamos a iterar a través de todos tweets de algún 'user' (¡argumento de la función!)\n",
        "    for tweet in tweepy.Cursor(api.user_timeline, screen_name = username).items(number_of_tweets):\n",
        "        #Armo una matriz con la información: \n",
        "        #                       usuario       id            fecha                 texto\n",
        "        tweets_for_csv.append([username, tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\")]) \n",
        "\n",
        "    # Lo paso a un archivo csv\n",
        "    outfile = username + \"_tweets.csv\" # string variable\n",
        "    print(\"Escribiendo en \" + outfile)\n",
        "    \n",
        "    # Vamos a trabajar con el comando 'with' que nos proporciona una sintaxis más limpia para trabajar con objetos de archivo. \n",
        "    # Una ventaja de utilizar este método es que los archivos abiertos se cerrarán automáticamente una vez que haya terminado.\n",
        "    with open(outfile, 'w+') as file:\n",
        "        writer = csv.writer(file, delimiter=',')\n",
        "        writer.writerows(tweets_for_csv)\n",
        "        \n",
        "    # Ojo: 'w+' abre un archivo para escribir y leer. Sobrescribe el archivo existente si el archivo existe. \n",
        "    #       Si el archivo no existe, crea un nuevo archivo para leer y escribir.\n",
        "    \n",
        "\n",
        "# Vamos a buscar los tweets de los profesores y departamento de Economía:\n",
        "users = ['wsosaescudero', 'Tommy_E_Murphy', 'EconUdesa']\n",
        "\n",
        "for user in users:\n",
        "    get_tweets(user)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kcQLtr2QFsOW"
      },
      "source": [
        "### Usando #Hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJtZMUtQGpIK",
        "colab": {}
      },
      "source": [
        "import tweepy\n",
        "import csv\n",
        "\n",
        "consumer_key = '2MlfpmBC4d8zzz1cZxM1DhxMm'\n",
        "consumer_secret = 'QxcmeHECRZhGRZ0HqkNglxT5SOAg3y4jRnGaGw6j2fxOmM3f16'\n",
        "access_token = \"1158370347089113088-dTsbvEWhDnKBwSpSXG9MnHHIn7HKHl\"\n",
        "access_token_secret = \"0QxtTRAfl6FLuIuBQxFUPQ3JxdnyBfjuGcZkDUowIYW7S\"\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "\n",
        "csvFile = open('ua.csv', 'a')\n",
        "#Use csv Writer\n",
        "csvWriter = csv.writer(csvFile)\n",
        "\n",
        "for tweet in tweepy.Cursor(api.search,q=\"#Obama\",count=100,\n",
        "                           lang=\"en\",\n",
        "                           since=\"2020-08-12\").items():\n",
        "    print (tweet.created_at, tweet.text)\n",
        "    csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "65Ino6tA-67B"
      },
      "source": [
        "### Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gdg8JddB_ONx",
        "colab": {}
      },
      "source": [
        "# pip install textblob\n",
        "\n",
        "import textblob\n",
        "from textblob import TextBlob\n",
        "\n",
        "# ¿Analizar texto? ¿Qué tipo?\n",
        "\n",
        "# Cada palabra del léxico tiene puntuaciones para: \n",
        "# 1)     polarity: negative vs. positive    (-1.0 => +1.0)\n",
        "# 2) subjectivity: objective vs. subjective (+0.0 => +1.0)\n",
        "\n",
        "wiki=TextBlob(\"Este tutorial es genial\")\n",
        "wiki.tags\n",
        "wiki.words\n",
        "wiki.sentiment.polarity\n",
        "\n",
        "wiki2=TextBlob(\"This tutorial is awesome\") # ¿Qué pasa si agrego 'not'?\n",
        "wiki2.tags\n",
        "wiki2.words\n",
        "wiki2.sentiment.polarity\n",
        "\n",
        "text = '''The teacher of this class is quite strange. She keeps\n",
        "telling us that what she's doing is awesome, but I find it rather\n",
        "boring. I will try to scape and get some coffee at the buffet.\n",
        "'''\n",
        "\n",
        "blob = TextBlob(text)\n",
        "blob.tags       \n",
        "blob.sentences\n",
        "                \n",
        "for sentence in blob.sentences:\n",
        "    print(sentence.sentiment.polarity)\n",
        "\n",
        "blob.translate(to=\"es\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TH0PSgmIQ7O",
        "colab_type": "text"
      },
      "source": [
        "#### Analizando tweets..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v_85HJKbDMjn",
        "colab": {}
      },
      "source": [
        "import tweepy\n",
        "\n",
        "consumer_key = '2MlfpmBC4d8zzz1cZxM1DhxMm'\n",
        "consumer_secret = 'QxcmeHECRZhGRZ0HqkNglxT5SOAg3y4jRnGaGw6j2fxOmM3f16'\n",
        "access_key = \"1158370347089113088-dTsbvEWhDnKBwSpSXG9MnHHIn7HKHl\"\n",
        "access_secret = \"0QxtTRAfl6FLuIuBQxFUPQ3JxdnyBfjuGcZkDUowIYW7S\"\n",
        "\n",
        "def get_tweets(username):\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_key, access_secret)\n",
        "    api = tweepy.API(auth)\n",
        "    \n",
        "    #¿Cuántos tweets?\n",
        "    number_of_tweets = 1\n",
        "\n",
        "    #¡Tomemos esos tweets!\n",
        "    for tweet in tweepy.Cursor(api.user_timeline, screen_name = username).items(number_of_tweets):\n",
        "        analysis=TextBlob(tweet.text)\n",
        "        print(analysis)\n",
        "        print(analysis.sentiment)\n",
        "        \n",
        "users_less_famous = ['realDonaldTrump', 'BarackObama']\n",
        "\n",
        "for user in users_less_famous:\n",
        "    get_tweets(user)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "swP3ukp5WdQH"
      },
      "source": [
        "### Twitter sin API: Web scraping\n",
        "\n",
        "Podemos no usar la API\n",
        "\n",
        "Y esto, queridos alumnos, es hacer web scraping!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2mOII7PTWn6q",
        "colab": {}
      },
      "source": [
        "# Para contar el número de tweets de una cuenta en particular:\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import requests\n",
        "\n",
        "user_agent_phone = 'Mozilla/5.0 (iPhone; CPU iPhone OS 5_1 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9B179 Safari/7534.48.3'\n",
        "\n",
        "headers = { 'User-Agent': user_agent_phone}\n",
        "\n",
        "handle = input('Nombre de la cuenta de Twitter: ')\n",
        "temp = requests.get('https://twitter.com/'+handle, headers=headers)\n",
        "bs = BeautifulSoup(temp.text,'lxml')\n",
        "\n",
        "try:\n",
        "    tweets = bs.find('td',{'class':'stat'}).find('div',{'class':'statnum'}).text\n",
        "    print(\"{} publicó {} tweets y comentarios.\".format(handle,tweets))\n",
        "\n",
        "except:\n",
        "    print('Nombre de cuenta no encontrado.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X2DMAfrqdrMR"
      },
      "source": [
        "Pero no todo es Twitter en la vida. Podríamos querer bajar precios de supermercados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s09ikETlDplG",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os \n",
        "import urllib.request\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "\n",
        "lista = ['frutas','verduras']\n",
        "\n",
        "name_product = []\n",
        "url_fruits = []\n",
        "price = []\n",
        "\n",
        "for i in lista:\n",
        "    url = \"https://www.jumbo.com.ar/\" \n",
        "\n",
        "    driver = webdriver.Chrome('C:\\\\Program Files\\\\ChromeDiver\\\\chromedriver.exe')\n",
        "    # Para descargar chromedriver executable entrar al siguiente link: https://sites.google.com/a/chromium.org/chromedriver/home\n",
        "    driver.get(url)\n",
        "    time.sleep(1)\n",
        "    driver.get(url+'frutas-y-verduras/{}'.format(i))\n",
        "\n",
        "    time.sleep(1)\n",
        "    elem = driver.find_element_by_tag_name(\"html\")\n",
        "\n",
        "    no_of_pagedowns = 50\n",
        "    while no_of_pagedowns:\n",
        "        elem.send_keys(Keys.PAGE_DOWN)\n",
        "        time.sleep(.5)\n",
        "        no_of_pagedowns-=1\n",
        "        \n",
        "    html = driver.page_source \n",
        "    soup = BeautifulSoup(html) \n",
        "     \n",
        "    data = soup.findAll('li',attrs={'layout':'1579df47-6ea5-4570-a858-8067a35362be',\n",
        "                                     'class':'frutas-y-verduras-ofertas-y-descuentos-en-frutas-y-verduras-|-jumbo'})\n",
        "    for elm in data:\n",
        "        title = elm.find('a',{'class':'product-item__name'}).get('title')\n",
        "        link = elm.find('a',{'class':'product-item__name'}).get('href')\n",
        "        try:\n",
        "            precio = elm.find('span',{'class':'product-prices__value product-prices__value--best-price'}).text\n",
        "        except:\n",
        "            precio='None'\n",
        "         \n",
        "        name_product.append(title)\n",
        "        url_fruits.append(link)\n",
        "        price.append(precio) \n",
        "    \n",
        "    driver.close()\n",
        "    driver.quit()\n",
        "\n",
        "fruits_vegsdf = {'nombre':name_product,'precio':price,'link':url_fruits}\n",
        "df = pd.DataFrame(data=fruits_vegsdf)\n",
        "writer = pd.ExcelWriter('fruits_vegsdf.xlsx', engine='xlsxwriter')\n",
        "df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
        "writer.save() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bOMyzkikaKjb"
      },
      "source": [
        "## EJERCICIOS (Tarea 2)\n",
        "**Fecha de entrega**: Miércoles 26/08 hasta las 23:59 hs. \n",
        "\n",
        "**Reglas de presentación**: \n",
        "* Enviar el código de Python por mail a csrebotroeder@udesa.edu.ar con el asunto \"Ejercicios tutorial 2 - APELLIDOS\". Por cada 5 minutos tarde en la entrega, se restará 1 punto sobre 10.\n",
        "* El trabajo debe elaborarse en grupos de **tres** personas. Cada grupo debe entregar **un solo trabajo**. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rS3kiioRbVAN"
      },
      "source": [
        "**EJERCICIO 1:**\n",
        "\n",
        "Escribir un código que guarde en un csv el usuario, la fecha de creación, el texto y la cantidad de veces que se le dio like a los últimos 75 tweets de Walter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4STTm5x5BXch"
      },
      "source": [
        "**EJERCICIO 2:**\n",
        "\n",
        "* ¿Para qué sirve este código? \n",
        "* ¿Qué pondría en el lugar de las X en el print() dentro de try?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "waJl1tD5W9Xe",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup, Comment\n",
        "import requests\n",
        "\n",
        "user_agent_phone = 'Mozilla/5.0 (iPhone; CPU iPhone OS 5_1 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9B179 Safari/7534.48.3'\n",
        "\n",
        "headers = { 'User-Agent': user_agent_phone}\n",
        "\n",
        "handle = input('Nombre de la cuenta de Twitter: ')\n",
        "temp = requests.get('https://twitter.com/'+handle, headers=headers)\n",
        "bs = BeautifulSoup(temp.text,'lxml')\n",
        "\n",
        "try:\n",
        "    mat = bs.find_all('div',{'class':'statnum'})\n",
        "    seg1 = mat[1].text\n",
        "    seg2 = mat[2].text\n",
        "    \n",
        "    var = bs.find('div',{'class':'bio'}).text\n",
        "    var = var.rstrip()\n",
        "    var = var.lstrip()\n",
        "\n",
        "    print('''\\n\\n{} XXXXX {} XXXXXXXX.\n",
        "          \\n{} XXXXX {} XXXXXXXX.\n",
        "          \\nXX XXXXXXX XX {} XX {}'''.format(handle,seg1, handle, seg2, handle, var))\n",
        "\n",
        "except:\n",
        "    print('Nombre de cuenta no encontrado.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qJntS7_uEn_G"
      },
      "source": [
        "**EJERCICIO 3:**\n",
        "\n",
        "El siguiente código es una continuación del código de scraping del supermercado Jumbo. \n",
        "* Explique **detalladamente** para qué sirve cada línea de código y cuál es la utilidad del mismo.\n",
        "* ¿Cuáles son los resultados que obtengo?\n",
        "* ¿Qué representan las listas 'W' y 'Y'?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1k-TFJ2lEoRl",
        "colab": {}
      },
      "source": [
        "file = 'fruits_vegsdf.xlsx'\n",
        "sheetname=\"Sheet1\"\n",
        "\n",
        "os.chdir(dir)\n",
        "cwd = os.getcwd()\n",
        "cwd\n",
        "\n",
        "ylist = []\n",
        "wlist = []\n",
        "\n",
        "xl = pd.ExcelFile(file)\n",
        "df = xl.parse(sheetname)\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    urlpage =  row['link'] \n",
        "    \n",
        "    try: \n",
        "        page = urllib.request.urlopen(urlpage)\n",
        "        soup = BeautifulSoup(page, 'html.parser') \n",
        "           \n",
        "        y = soup.find('div',{'class':'skuReference'}).text         \n",
        "        w = soup.find('span',{'class':'brand'}).text         \n",
        "    except:\n",
        "        y = 'None'\n",
        "        w = 'None'\n",
        "        \n",
        "    ylist.append(y)\n",
        "    wlist.append(w)\n",
        "\n",
        "df['Y'] = ylist\n",
        "df['W'] = wlist\n",
        "df = df[['nombre', 'Y', 'W', 'precio', 'link']]\n",
        "writer = pd.ExcelWriter('fruit and vegs results.xlsx', engine='xlsxwriter')\n",
        "df.drop_duplicates(keep='first')  .to_excel(writer, sheet_name='Sheet1', index=False)\n",
        "writer.save() "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}